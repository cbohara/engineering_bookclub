Planning and Designing Databases on AWS

Intro

CAP theorem - only get 2 out of 3
C - consistency - ensuring all systems produce the same response
A - availability - ensuring that every request receives a response
P - partition tolerance - ensuring that the system can survive the loss of a partition - must have in distributed systems

CA - consistency + availability - possible on a single system but not realistic
AP - available + partition tolerance - distributed nonrelational database systems - how do we recover without consistency after node failures?
CP - consistency + partition tolerance - sacrifice availablity for consistency - like financial data - easier to design

PIE theorem - only get 2 out of 3
P - pattern (query) flexibility - ensuring that the system supports random access patterns and one-time queries - Do you want the option to ask new, unanticipated questions of your data?
I - infinite scale - ensure that the system can gracefully scale without practical limits
E - efficiency - ensuring that the system delivers required latency at all times

PE - pattern flexibility + efficiency - relational databases
EI - infinite scale + efficiency - systems where pattern flexibility is sacrified in favor of a highly scalable efficient app - NoSQL
PI - pattern flexilibity + infinite scale - systems where efficiency is sacrified in favor of a flexible + scalable app - data warehouse 

transactional compliance
transaction = logical unit of commands that are executed against a database
if I am changing a piece of data, and someone else comes along and tries to change the same piece of data, what happens?

ACID compliance
A - atomicity - all or nothing - complete success or complete failure + rollback
C - consistency - traditionally applies to relational databases via constraints
I - isolation - one transaction cannot interfere with another - accomplished via locks - write lock = other person cannot change at the same time - pesimmistic type of lock = avoid anyone else doing anything else when locked down
D - durability - ensure that changes are permanent - traditionally in the transaction log

BASE compliance
BA - basically available  - changes on one system are made available immediately
S - soft state - partial consistency is accepted
E - eventually consistent - all systems will eventually receive changes

Workload requirements
Data storage - What type of data store?
Data usage - How will the data be used?
Data volume - How much?
Data velocity - How quickly will it change?
Data variety - How different is the data from one input to the next? 
Data longevity - How cold does the data get? How long do we need to store it for quick access?
Data durability - knowing that the data is always there, avoid user error
Data availability - knowing your data will be accessible when needed in the worse case scenario - by avoiding single points of failure, you incur cost

Design considerations
performance - latency for reads? read/write throughput? what percentage of the traffic coming in is read vs write? perhaps offload reads to a read replica and focus writes on the main node. how much concurrency are we managing?
scaling - autoscale to meet capacity
sizing - determine the appropriate instance type + scaling vert vs horiz
high availablity - read replicas, clustering, multi-AZ deployment, multi-region deployment
networking - VPC configs - public/private subnets 
backup + recovery - how much data can be lost? how quickly must the system be operational? design for failure
security + compliance - how should the data be secured? how do we adhere to compliance?

Why use nonrelational?
Trade-offs you want to make:
Speed over consistency
Flexible over strict schema
Native document storage vs data normalization
Relationship nevigation over freeform querying
Speed over persistence

RDS high availability 
Only sychrononous replication occurs to secondary instance - all other replication to read replicas is async

How traditional RDS databases work in monolithic architecture
Page read from disk and brought into memory for a user to update
Once user has updated the page, the page is considered dirty because memory doesn't match disk
Engine doesn't immediately write dirty pages to disk
Engine instead optimizes by writing dirty pages to disk in batches via process of checkpointing 
Tracks via transaction logs 

What Aurora offers as cloud native DB
Cluster volume is the same across all the instances that exist - essentially the transaction log is now the database
Do fewer I/Os
Minimize network packets
Cache prior results
Offload the database engine by decoupling storage and compute - data is independent of database instance

Aurora MySQL parallel query clusters
Allows queries to run in parallel
Great for OLAP efficiency 
Boost performance on very large or complex operations

Aurora global database
1 single master writer 
Can have up to 15 read replicas per region, up to 5 regions
Always good to have at least 1 read replica in another AZ for failover

Aurora serverless
https://aws.amazon.com/about-aws/whats-new/2020/11/aws-database-migration-service-now-supports-aurora-postgresql-serverless/

DynamoDB
Global tables
Quickly and easy add regions 

DynamoDB DAX
cache cluster between DynamoDB and client
client only has to point to DAX and if there is a hit in the cache, then no need to read from DynamoDB, else go read from DynamoDB directly

estimating RCU and WCU
set to on demand and monitor to determine appropriate RCU + WCU
later provision based on RCU and WCU during on demand mode

Neptune demo
https://aws.amazon.com/blogs/database/let-me-graph-that-for-you-part-1-air-routes/

Redis
Built in geospatial commands
https://redislabs.com/redis-best-practices/indexing-patterns/geospatial/
Smarter recovery in case shards go down vs Memcache
Common use case is to have a side cache, but some argue you can use it as your main database

What ElastiCache offers
Real time data access 

Which ElasticCache engine and config would enable scaling vertically with no data loss nor downtime
C - ElasticCache for Redis, cluster-mode enabled

Redshift w/ Spectrum
Keep raw data in S3 to query via Spectrum
Keep aggregated data in Redshift for reporting

Big advantage of Redshift over Athena these days is the workload management
Redshift allows for different priority queues vs Athena only has a single queue per account
