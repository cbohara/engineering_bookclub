AWS Reinvent Conference 

##############
MONDAY
##############

##############
12:15pm
ANT204
How Amazon leverages AWS to deliver analytics at enterprise scale
##############

Project Andes
Amazon.com analytics transition from Oracle DW > AWS

ELT = pure SQL statements
(only) had to migrate 1M SQL queries
didn't have to worry about other programming languages in the ecosystem

limited the scope to only structured data + users can query via SQL

EMR allows you to decouple storage + compute = ideal
Redshift = still coupled compute + storaged
however Redshift allows easy querying via SQL

store all data in S3
add layer (catalogue) on top of S3
replicate database like experience

allow users ability to transform/join datasets 

synchronized metadata
built subscription service that would activate ETL based on updates

redshift spectrum = glue + redshift

how to get people off a legacy system on to a new system
standup replica of the warehouse
allow users to manage their own migration strategy timeline

schema conversion tool from AWS
automated query conversion from Oracle to Redshift SQL syntax 

Andes metadata service
any time data changed > synchronize downstream jobs
completion service = track if a job is done
manifests = abstract away file updates to optimize for later reads
audit logging for changes

subscription service
contract = publisher determined governance + SLAs
synchronization = updates data/metadata/schema

synchronizer targets
1. Redshift
2. AWS Glue catalog
fully decoupled storage + compute
3. Redshift spectrum 
launched halfway through migration
turns fully coupled + compute into semi-coupled  
run query against S3 as if running on local Redshift cluster
gained significant new functionality with minimal investment

need to be ready when working in the AWS environment to always update technology

Andes user experience
give full metadata via UI
provide data completness updates
provide schema attributes
enable user to subscribe to a dataset

migration context
Amazon.com does not stop
make sure users have necessary training 

running 2 systems in parallel is super expensive
move users from old system to new system as fast as possible

need to transition 80K users
over-communicate often

Andes has been taking the full analytics workload for Amazon.com for the past 12 months

leverage data migration service (DMS) + schema conversation tool (SCT)


##############
2:30pm
WPT203
We Power Tech - In Diversity + Inclusion, Details Matter
##############

data in diversity + inclusion
ask HR to keep metrics beyond recruitment
keep metrics at a team level

ex: data around how long a population is at a certain level (1 vs 2 vs senior vs staff)
how quickly do they rise up the ranks?

rates of women in CS degrees has decreased(?!)
half of women are likely to leave the field within the first 7 years

bias training is not effective
you can't change a company culture by doing anything once

what can you do to change things?
listen to what other people are staying
apply compassion
hold people accountable 
bias interuptor tool for reviews
https://biasinterrupters.org/toolkits/individualtools/
sponsors - help individuals grow their career

##############
TUESDAY
##############

##############
11:30am
ANT205
What's new with Athena chalktalk
##############

in preview = can query via RDS + DynamoDB

Athena workgroups
more data in data lake > better isolate workload

IAM roles supported
cannot currently limit per specific user - now at the group level
cost allocation tags per workgroup

automated athena queries
can get notified via SNS if what to know if data scans start to creep up

output alternatives to current csv via console
can create user-defined functions now
can currently use SDK + CLI
results of Athena query to Lambda function

JDBC connector to BI tool to avoid timeouts/console issues

query > queue
query shows as running but not actually running
noticing that jobs running slower at the top of the hour - move to middle of the hour

Lake Formation
used to control access permissions per user
even at the column level control, may be row level in the future
facilitates auditing

rumors to automate limiting PII data access 

if underlying data is not columnar based, then even if you only show the user a specific column they are allowed, all data is still scanned
good use case to opt for parquet files moving forward

PrivateLink = private endpoint

question
noticed that queries are slowing down as more and more ad-hoc + automated in a given workgroup 
does each workgroup have it's own queue?

S3 requester = can query s3 in a different account?

in preview = can request to use the new version of presto
better support for geospatial functions

UDF
custom code in SQL queries
Query Federation SDK

ask max increase

https://aws.amazon.com/blogs/big-data/query-any-data-source-with-amazon-athenas-new-federated-query/
SQL queries across data stored in relational, non-relational, object, and custom data sources
Athena executes federated queries using Data Source Connectors that run on AWS Lambda

##############
3:30pm
ANT222
Analytics with Amazon Athena
##############

https://aws.amazon.com/blogs/big-data/query-any-data-source-with-amazon-athenas-new-federated-query/

only available in us-east-1
use specific workgroup when wanting to use new functionality
AmazonAthenaPreviewFunctionality

now can use Athena to query 10 new data sources
build your own UDFs
build your own connectors to custom data sources
apply sagemaker inference as part of your queries
100% open sourcee SDK + connectors

https://github.com/awslabs/aws-athena-query-federation

workshop overview
intro to Athena Federated Query
connect Athena to Amazon Cloudwatch logs
build custom data source
build UDF
use sagemaker

avoid data silos
you can still choose best data store for specific data problem
now query it all from Athena

connectors
- JDBC
Aurora, MySQL, Postgres, Redshift
- Redis  
- CloudWatch Logs + metrics
- CMDB
select * for ec2 instances;

Athena query > lambda function > data sources
built on top of Apache Arrow

Athena will avoid stressing the data source if running read query will hurt your system

1. Connect Athena to Amazon Cloudwatch logs
https://github.com/awslabs/aws-athena-query-federation

https://console.aws.amazon.com/lambda/home?region=us-east-1#/create/app?applicationId=arn:aws:serverlessrepo:us-east-1:292517598671:applications/AthenaCloudwatchConnector
Navigate to Servless Application Repository and search for "athena-federation".
Be sure to check the box to show entries that require custom IAM roles.
Look for entries published by the "Amazon Athena Federation" author.

We used CloudwatchConnector
Provide S3 bucket
AthenaCatalogName
The name you will give to this catalog in Athena. !!It will also be used as the function name!!
Deploy the application

Go to the Athena Console in us-east-1 (N. Virginia) and create a workgroup called "AmazonAthenaPreviewFunctionality", 
any queries run from that workgroup will be able to use Preview features described in this repository.
Run a query "show databses in `lambda:<func_name>`" where <func_name> is the name of the Lambda function you deployed in the previous steps.

2. Create custom connector
https://github.com/awslabs/aws-athena-query-federation/tree/master/athena-example

lesson learned
I am DEFINITELY NOT going to create a custom connection nor UDFs :)
too involved

3. ML in Athena
https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-athena-adds-support-for-invoking-machine-learning-models-in-sql-queries/
https://github.com/awslabs/aws-athena-query-federation/wiki/AthenaML-Tutorial

##############
WEDNESDAY
##############

##############
8:30am
ANT237
Redshift workshop
##############

architecture
columnar database

leader node
connect via JDBC connection
only takes SQL command and requests work from compute nodes

compute nodes
special ability to read from S3
massively parrallel fashion

redshift spectrum
10x the processing power available when querying via redshift spectrum 
no data loading required
scale compute + storage separately
directly query data stored in S3
parquet, ORC, avro, JSON, CSV data formats
spectrum request 

redshift advisor
provides suggestions/guidance

5 points of guidance = SET DW
S
sort key
improve filter performance
choose up to 3 columns

E
encoding of columns
compress all columns except for 1st sort key column

T
table maintainance
auto vacuum

D
distribution key

W
warehouse management
setup different queues = one for ETL, one for ad-hoc
can setup query monitoring rules
ex: stops user from running select *

concurrency scaling
have redshift spin up additional clusters when queue backs up

elastic resize
size cluster up and down based on predictable trends

https://github.com/mrscaer/amazon-redshift-modernize-dw
compression
when using the copy command to insert data into an empty table, Redshift will auto determine the compression per column based on the data type of the column 

distribution
Redshift uses a block size of 1 MB

an optimal distribution = even # of rows per node
if the distkey contains few unique values (low cardinality) or any excessive repeating values (non-uniform) then some nodes receive too many/too little rows

use the ALTER table command to change the distribution key

dist style AUTO is the default
good for getting started with small data sets
later on may be best to explicitly choose a dist style that best fits the dataset

best practice!
after inserting or deleting lots of rows from a table, run analyze and vacuum to update the statistics on the table + cleanup "deleted" rows that are actually still around

sort key
implement sort key in order to minimize data scan per query

workflow management
create different groups with different queues

admin scripts 
https://github.com/awslabs/amazon-redshift-utils
https://github.com/awslabs/amazon-redshift-utils/tree/master/src/AnalyzeVacuumUtility

spectrum tutorial help
https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html

what was missing? IAM role missing full S3 permissions
https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum-create-role.html

what went wrong? including table properties ('numRows'='172000')
https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum-create-external-table.html

to read 
https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/

##############
11:45am
ANT206-L
Leadership session: Trends with data lakes and analytics
##############

data lake
cost effective
open to all users/applications to leverage data as broadly as possible
use parquet for best analytics performance
100gb/sec off of s3 = awesome network bandwidth
data swam > Glue/Lake formation for cataloging > data lake

lake formation
ingest, catalog, and provide access control at the column level
central point of configuration

glue
data catalog
ETL = developer mindset = serverless spark

New Relic - Kinesis integration 
https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-kinesis-streams-monitoring-integration

AWS data exchange
https://aws.amazon.com/data-exchange/
marketplace around data sets
Life360 can become a publisher of data sets
enrich your data with 3rd party data

Redshift
not just a data warehouse anymore
now the lake house
how do you get the best price performance?
200+ new features in the past 18 months
past few weeks = new console experience, materialized views, evaluate spacial data, 2.3x performance improvement
https://aws.amazon.com/redshift/whats-new/

Redshift federated query (preview)
common pattern = historical data in data lake, recent data in data warehouse
extend redshift to operation databases (Aurora, Postgres) = get real time business data

redshift query optimizer will try to rewrite queries behind the scenes for better performance

new RA3 instance
https://aws.amazon.com/about-aws/whats-new/2019/12/amazon-redshift-announces-ra3-nodes-managed-storage/
ability to pay for compute + storage separately

redshift AQUA 
https://techcrunch.com/2019/12/03/aws-speeds-up-redshift-queries-10x-with-aqua/ 
process query with more nodes than are available in your cluster

Redshift spatial support
https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-redshift-announces-support-spatial-data/
https://aws.amazon.com/blogs/aws/using-spatial-data-with-amazon-redshift/

EMR
spot instances 

Elasticsearch service
UltraWarm = warm storage tier for ES

Athena
super simple SQL querying over S3
federated querying = very different between Athena + Redshift

##############
8:30am
ANT404
Build a single query to analyze data across Amazon Redshift & Amazon S3
##############

objectives

- query log files + nested JSON data in S3
- create shared data catalog using AWS glue
- export data in columnar format using unload
- create a secure data lake using formation
- new geospatial functionality

redshift + spectrum recap

redshift architecture
introduced new clusters on the fly to avoid backing up queues
provide 1 hr per day to try out
question - is there a cost control management system?

new Redshift features
vacuum + analyze maintenance runs automatically in the background
workload management = better manage resources, not wasting away on an unused queue
take data available in redshift + unload into s3 as parquet files 

Spectrum
don't load data into redshift
create a table over external data store in S3
querying in place

spectrum query processing
query optimized + compiled on leader node
determine what goes to redshift spectrum + what is taken care of by redshift compute node
query plan sent to compute nodes

new Spectrum features
speed up queries by caching metadata + small returns
nested data support

jupyter notebook will run redshift queries as if run from the console using the aws cli 

AWS glue database is mapped to redshift spectrum schema

-- querying JSON format cloudtrail via redshift spectrum
https://docs.aws.amazon.com/redshift/latest/dg/tutorial-query-nested-data.html
scalar object = single value
vs objects + arrays = multiple values
fixed DDL! will need to define everything upfront like we do in Athena
question - is there anything like the glue crawler?  yes - will be taking care of later

-- unload into parquet
helpful for unloading colder data from Redshift DW into a data lake
best to have large files ~128MB

RE3 with managed storage = new type of redshift node
transparently offload data on your behalf
do not have access to the underlying S3 data

-- lake formation
IAM can be tedious
lake formation adds permissions layer on top of your data lake
however right now it's painful to use

-- spatial queries
