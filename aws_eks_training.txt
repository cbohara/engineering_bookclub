problems with virtual machines - lots of resources in each VM that aren't necessary
container solution - don't need a full blown OS, share the underlying computer's OS
very lightweight, can have many containers on a single machine, fast to spin up
more secure bc less things running = less attack surface = less likely to get hacked

Twelve-factor app
portability - easily works from laptop to cloud 
how to scale? simply add another container
continuous deployment - work in dev, then run in QA test env, then deploy in prod

monolith vs microservice
monolith disadvantages
- one bug in one component will have cascading impact in other components 
- can't scale different components independently

microservice advantages 
- decouple components into seperate APIs
- easily reuse components 

Dockerfile = file in which we write template details
image = read-only template that contains a set of instructions for creating a container that can run on the Docker platform
container = running instance of an image
if you must update the application code or apply a patch, you build a new image and redeploy it

yum -y update = update OS 
yum -y install httpd = install apache web server
expose port 80 = HTTP
expose port 443 = HTTPS
COPY shell script into container
RUN shell script = install applications
CMD = starts the application 
WORKDIR = set working directory

don't need to use a more abstract ubuntu instance
keep images as simple as possible
if I have a python app, use a python image
lighter images = more efficient and more secure

EC2 - launch instance as jump box
security group = firewall = monitors incoming/outgoing traffic and permits/blocks data packets based on security rules

create image from Dockerfile
docker build -t (tag) webserver:v1 . (current dir)

multi-stage Dockerfile
at one stage, creates an image
second stage, instead of using a big root image, just use smaller image
advantage = generates smaller images
read more:
https://docs.docker.com/develop/develop-images/multistage-build/

repository = place to store images 
Amazon Elastic Container Registry (ECR)
every repo has a URI 
step 1 - tag local image with ECR URI
docker tag webserver:v1 <ECR_URI>:webserverv1
ex: docker tag webserver:v1 012345678910.dkr.ecr.us-east-1.amazonaws.com/webserverv1:webserverv1
step 2 - push image to ECR
docker push 012345678910.dkr.ecr.us-east-1.amazonaws.com/webserverv1:webserverv1

kubernetes 
orchestrates containers 
pods = layer of abstraction - containers are deployed in a pod
volumes = data storage
services = front end for multiple pods

Kubernetes manifest files = PodSpecs
kind: Job
https://kubernetes.io/docs/concepts/workloads/pods/
cannot simply point to existing Dockerfile
need to rewrite container details to meet PodSpec guidelines

ConfigMaps = config files
https://v1-19.docs.kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/
kind: ConfigMap
have sep ConfigMap for dev and prod

kind: Secret
need to base64 passwords before entering into the config
K8 then encrypts secrets file 

kind: Deployment 
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
specify deployment details

pod scheduling
volume filter - satisfying volume requirements + contraints 
resource filters - satisfying resource rquirements = including min/max CPU/memory
topology filters - ex specify types of EC2 nodes

DaemonSet
worker nodes = EC2 instances that run pods

Fluentd - log aggregator
https://www.fluentd.org/
can forward logs to Cloudwatch Logs

custom resource definition

control plane and data plane
control plane - master node
- control manager = driver program = decision maker
- cloud controller
- scheduler = YARN
- API server - interacts with worker nodes
- etcd = stores changelogs of changes to cluster

data plane - worker nodes
- kube-proxy = manages networking
- docker = runtime 
- kubelet = communicates with control plane specifically API server

how to communicate with cluster? kubectl 
https://kubernetes.io/docs/tasks/tools/
ssh into jump box and run commands to cluster
shows the delta between current + desired state

Namespace
Kubernetes supports multiple virtual clusters backed by the same physical cluster
These virtual clusters are called namespaces
Names of resources need to be unique within a namespace, but not across namespaces.
easily share resources within a namespace
not necessarily between namespaces

AWS EKS
managed K8 service
EKS takes care of the control plane
User has the option to either
- fully manage worker nodes as EC2 instances
- fully turn over management as Fargate instances
- goldilocks option = managed node group

min node size = t2.medium, else may run into errors

eksctl create cluster \ 
--name EKS1 \
--region us-west-2 \
--version 1.18 \
--nodegroup-name eks-cluster-workers \
--node-type t2.medium \
--nodes 2 \
--nodes-min 2 \
--nodes-max \
--with-oidc \
--managed \
--enable-ssm

by default EKS is deployed into a VPC

if a worker node has an issue, it will be noted as degraded

AWS Fargate 
does not use EC2 instances under the hood
uses microVM - Firecracker 
https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/

EC2 instance = billed per instance
Fargate = per resources that a pod uses

need to specify Fargate profile
if Fargate profile is not specified, will default to managed nodes on EC2 instances
the K8 deployment script label needs to match fargate 

security fundamentals
authentication = prove who you are (ex: user ID, API token)
authorization = what am I allowed to do?

kubectl get nodes > K8 API 
authentication - managed by AWS - IAM
authorization - managed by K8 - role based access control

kind: ConfigMap
aws-auth
maps IAM role/user to K8 users/groups/service

create EKS cluster
* kubectl create namespace games
* eksctl create fargateprofile

deploy app
create deployment.yaml
kind: Deployment
deploy application using Fargate

* kubectl apply -f deployment.yaml
* kubectl get pod 
show the pods
* kubectl describe pods <POD NAME>
need to specify namespace in commands -n, otherwise all commands are ran against default namespace

will receive a cheat sheet later 

to config and mange clusters use 

console path
create IAM role and attach
simply deploy via 
https://aws.amazon.com/quickstart/architecture/amazon-eks/

steps:
spin up EC2 instance
download IAM authenticator
download kubectl
create kubectl config file

permissions
create ec2 instances = IAM
control inbound + outbound traffic to + from nodes = security groups
connect to the cluster API server = kubeconfig file
add a worker node to the cluster = IAM role + kubeconfig file

ECR - container image repository
Helm - package manager
AWS CodePipeline - CI/CD pipeline

kubectl = K8 cli tool
eksctl = AWS EKS CLI tool

https://kubernetes.io/docs/concepts/overview/components/
when you deploy K8, you create a cluster
each cluster contains at least 1 worker node
worker machines/nodes run container apps
worker nodes host the pods
pods = K8 wrapper around a set of running containers with shared storage and network resources
control plane manages the worker nodes (machine) + Pods (bunch of containers) in the cluster 

with initial deploy you set min and max worker node count for cluster
you an update the count later

managed node groups automate creation of EC2 instances for EKS cluster
middle option between manually creating EC2 instances and totally serverless Fargate option
shows up under the config - compute section
launches EC2 instances that are managed by an autoscaling group

in deployment config describe desired state 
use apply command to apply config to cluster
kubectl apply -f kubernetes/deployment.yaml

service = abstract way to expose an app running on a set of pods as a consolidated service
kubectl apply -f kubernetes/service.yaml

can view deployment state by running get command
kubectl get deployment ecsdemo-nodejs

view all deployments
kubectl get deployments

can scale out the deployment by specifying the number of pod (set of containers) replicas (number of copies of the set of containers)
kubectl scale deployment ecsdemo-crystal --replicas=3

AWS CodePipeline = general AWS GitOps tool
pipeline1 = build docker image and store in image reposity ECR (Elastic Container Registry)
pipeline2 = manage K8 manifest files = desired state of the app

Weave Flux = specific GitOps tool for K8 clusters
automatically ensures the state of K8 cluster matches configs in git
Helm = K8 package manager = interact with K8 config Gitub repo + manages K8 app
Charts = Helm packages

sudo wget -O /usr/local/bin/fluxctl $(curl https://api.github.com/repos/fluxcd/flux/releases/latest | jq -r ".assets[] | select(.name | test(\"linux_amd64\")) | .browser_download_url")

Service mesh
Enables networking between K8 services

Monitoring and controlling services
Enabiling nd-to-end visibility and high availability for apps is difficult
envoy service proxy - sidecar container used as a proxy for incoming/outgoing traffic to the primary microservice
AWS implemented the Envoy proxy as AWS App Mesh
App Mesh, using Envoy, allows you to have much better observability of apps + insight into networking needs

example app
dj app microservice requests list of artists from different backend genre specific services
doesn't automatically refresh backend service list

need to specify namespace in which we are working with to see the pods, deployment, and the services 
kubectl -n prodapp get pods,deploy,service

pod = group of containers deployed on the same host
deployment = resource object that controls the desired state of the pods 
service = group of pods running on the cluster

store pod name as environment variable
-l label so filter just by dj app
-o specify json output
DJ_POD_NAME=$(kubectl get pods -n prodapp -l app=dj -o jsonpath='{.items[0].metadata.name}')

can essentially SSH into the running container by running this command 
kubectl exec -n prodapp -it ${DJ_POD_NAME} bash

from the dj app we are making a request to the country backend service to get a list of country artists
curl -s country-v1:9080 | json_pp

https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
don't need to provide the IP address can simply provide the service name 
Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures the kubelets to tell individual containers to use the DNS Service's IP to resolve DNS names

create App Mesh-enabled versions of DJ app
first need to installl
Helm = package manager
deploy App Mesh controller
create OIDC provider for the cluster

App Mesh
virtual nodes = logical pointers to K8 services or APp Mesh virtual service
virtual services = abstraction of a real service provided by the virtual node
app mesh sidecar = container runs next to microservice configures your pods to use App Mesh traffic rules defined for virtual nodes or virtual routers
app mesh injector = installs as a webhook and injects App Mesh sidecar container images 

add EKS package to helm
helm repo add eks https://aws.github.io/eks-charts

EKS hosts a per cluster public OpenID Connect (OIDC) endpoint which contains signing keys for JSON web tokens
allows external systems like IAM to validate and accpet the K8 issued OIDC tokens
OIDC federation allows you to assume IAM roles via the Secure Token Service (STS)
enables auth with the OIDC provider, and recieve a JSON Web Token (JWT) which is used to assume an IAM role

identity provider = stores + authenticates (who am I) of users to ensure the validity of their login creds across multiple platforms (like google auth)
OpenID connect (OIDC) = auth layer (determines who I am) 

create OIDC ID provider for cluster
eksctl utils associate-iam-oidc-provider --cluster dev-cluster --approve --region ${AWS_REGION}
associate-iam-oidc-provider - allows IAM OIDC provider for cluster = enable IAM roles for pods

create IAM role for App Mesh Controller Service Account
attach IAM policy AWSAppMeshFullAccess using ekstl util

create namespace
kubectl create ns appmesh-system

create IAM role with attached IAM policy
eksctl create iamserviceaccount --cluster dev-cluster --namespace appmesh-system --name appmesh-controller --attach-policy-arn arn:aws:iam::aws:policy/AWSCloudMapFullAccess,arn:aws:iam::aws:policy/AWSAppMeshFullAccess --override-existing-serviceaccounts --approve --region ${AWS_REGION}

install AppMesh controller
helm upgrade -i appmesh-controller eks/appmesh-controller --namespace appmesh-system --set region=${AWS_REGION} --set serviceAccount.create=false --set serviceAccount.name=appmesh-controller
upgrade a release
if a release by this name doesn't exist already, run an install of the release
serviceAcount.create=false - don't create because we did so in the previous step
serviceAccount.name=set the service account name we created above 

deploy AppMesh namespace 
kubectl apply -f /home/ssm-user/djapp/2_app_mesh/namespace.yaml

enables sidecar injector 
kind: Namespace
appmesh.k8s.aws/sidecarInjectorWebhook: enabled

create a mesh
kubectl apply -f /home/ssm-user/djapp/2_app_mesh/mesh.yaml
apiVersion: appmesh.k8s.aws/v1beta2
kind: Mesh
metadata:
  name: dj-app
spec:
  namespaceSelector:
    matchLabels:
      mesh: dj-app

so all pods with label dj-app will be able to communicate on the mesh

create virtual nodes and services
required in order for AppMesh to work

use -n to filter by namespace
kubectl -n prodapp get virtualnodes

aws appmesh list-meshes

view all objects within namespace
kubectl get all -n prodapp

need to force pods to be recreated to inject Envoy proxy sidecars into the pods

restart deployment of all services incorperated with mesh
kubectl -n prodapp rollout restart deployment dj rock-v1 country-v1

Fluent bit = log collector daemon running on EKS cluster worker nodes
Routes data to Kinesis Firehose > Logs data to S3 > Query via Athena
Also can send to Amazon CloudWatch container insights 

create kinesis firehose

create service account fluent-bit for log aggregation
kubectl create sa fluent-bit

sh-4.2$ cat ~/scripts/task2/eks-fluent-bit-daemonset-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-log-reader
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - pods
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-log-crb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: pod-log-reader
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: default

create role and role binding for Fluent Bit to use
kubectl apply -f ~/scripts/task2/eks-fluent-bit-daemonset-rbac.yaml

create ConfigMap to define Fluent Bit log parsing and routing parameters
kubectl apply -f ~/scripts/task2/eks-fluent-bit-configmap.yaml

create Fluent Bit K8 daemonset
kubectl apply -f ~/scripts/task2/eks-fluent-bit-daemonset.yaml

check status of setting up daemon
kubectl get daemonset fluentbit

setup Container Insights using a quick start template
curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed "s/{{cluster_name}}/dev-cluster/;s/{{region_name}}/AWS_REGION/" | kubectl apply -f -

Security lab
integrate AWS IAM for authentification (who am I?) with K8 role based access control (RBAC) for authorization (what can I do?)

aws-auth ConfigMap
map EksNodeRole IAM role to system:bootstrappers and system:nodes K8 RBAC groups
EksNodeRole IAM role is applied to the EC2 instances
username in RBAC = system:node:ip-10-10-119-55.ec2.internal given the IP address

recommend to have a dedicated IAM role for creating EKS clusters + keep that info as secure as AWS root account
in the lab its the BastionHostIamROle

always create a backup copy just in case :) 
cp ~/.kube/config ~/.kube/config.back

kubectl create deployment nginx --image=nginx -n web
kubectl expose deployment nginx --port=80 --target-port=80 --name nginx --type=LoadBalancer -n web

*Connect K8 to other AWS APIs*
create IAM role with a trust relationship that allows K8 service to assume the role
create temp container that uses the service account + execute AWS CLI commands to access S3 APIs from within the container

steps:
1 - create IAM openID connect (OIDC) ID providere - allows you to create trust relationship between AWS account + K8 RBAC
eksctl utils associate-iam-oidc-provider --cluster dev-cluster --approve --region ${AWS_REGION}

2- create IAM role and associated K8 service account
can provide multiple --attach-policy-arn flags
eksctl create iamserviceaccount --name aws-s3-read --namespace default --cluster dev-cluster --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess --approve --region ${AWS_REGION}

3 - create new container that uses aws-s3-read service account
kubectl run my-shell --generator=run-pod/v1 --rm -i --tty --image amazonlinux --serviceaccount aws-s3-read -- bash

4 - install AWS CLI
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
yum install unzip less groff -y
unzip awscliv2.zip
./aws/install

5 - check connection with s3
aws s3 ls 
