Data Analytics Exam Readiness
https://www.aws.training/Details/eLearning?id=46612

# general exam strategy
read + understand ?
ID key phrases + qualifiers 
try to answer the question before reading answer options
elminate answer options based on key phrases and qualifiers 
if you still don't know, flag the question ad move on
make sure to answer all questions before time is up

all responses are plausible
must ID which meets the specific requirements 

Diagnose question
1. ID any AWS services and situational characterists
2. ID issues + concerns
3. ID requirements
4. ID what you are being asked for

########
Domain 1: Collection
########
ingesting raw data
wide variety - structured, semistructured, unstructured
from batch to streaming

collection topics:
ops characteristics
handles freq, volume, and source
addresses key properties of data - i.e. formmat, compression

###
Subdomain 1.1: Determine the operational characteristics of the collection system
###

kinesis data streams
1. what happens if a record fails?
KPL PutRecord often sends multiple records to the stream per request
if a single record fails, it will automatically be added back to the KPL buffer + retried
failure of one record doesn't impact the processing other records in the request

https://docs.aws.amazon.com/streams/latest/dev/kinesis-producer-adv-retries-rate-limiting.html
when you add KPL user record, a record is given a timestamp + added to a buffer with a deadline set by the RecordMaxBufferTime
the time stamp/deadline sets the buffer priority
records are flushed from the buffer based on
buffer priority - RecordMaxBufferTime
aggregration configuration - AggregationMaxCount, AggregationMaxSize 
collection configuration - CollectionMaxCount, CollectionMaxSize

records flushed are then sent to to Kinesis data stream using PutRecords
records that fail are automatically put back into KPL buffer
new deadline = whatever is the lower value
half the current RecordMaxBufferedTime
record's TTL value
this is an aggressive retry strategy

rate limiting 
default throughput per producer is 50% higher than the actual shard limit > allows for shard saturation from a single producer
best practice is for each producer to retry for max throughput aggressively 
handle any resulting throttling if determined excessive by expanding capacity of stream by resharing and implementing appropriate partition key strategy

https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/
for KPL to scale must
- batch records and perform parallel HTTP requests
- handle transient failures + perform retries

PutRecords = maximize throughput of sending records
PutRecords is not atomic - can partially fail
PutRecords always returns 200 even if some records within the batch failed
must write code that exams PutRecordsResult object to ID individual failures + take appropriate action

Aggregation = maximize throughput of shards receiving records
each shard can receive 1MiB or 1000 records/second and is throttled if either limit is reached

The KPL provides the following features out of the box:
Batching of puts using PutRecords (Collection) = max throughput sending records
Tracking of record age and enforcement of maximum buffering times
Per-shard record aggregation (Aggregation) = max throughput receiving records
Retries in case of errors, with ability to distinguish between retryable and non-retryable errors
Per-shard rate limiting to prevent excessive and pointless spamming
Useful metrics and a highly efficient CloudWatch client

2. how available + durable is kinesis data streams?
synchronously replicates data across 3 AZ
not suited for long term persistence - default retention of 24hrs, can extend to 7 days

3. what happens if a consumer fails?
KCL checkpointing 
stores a cursor in DDB to durably track records that have been read by the kinesis stream
bc of this feature you need to
have uniq app names in KCL for each app 
be mindful of DDB provisioned throughput errors if your stream has too many shards or if the application does frequent checkpointing

max retention for collection services
DDB streams = 24 hours
Kinesis data stream = 7 days 
SQS = 14 days

###
Subdomain 1.2: Select a collection system that handles the frequency, volume, and source of data
###

Use the right tool for the job
Focus on these 2 main questions
1. How quickly do you need analytics results?
2. Where is the data coming from

Batch - Glue
Streaming - Kinesis
Transactional - DMS (not only for one-time migration, can also be used for continuous replication) > S3 > Glue > Data Lake

Kinesis data stream
1,000 PUT/second per shard
1 share = 1 MB/sec input + 2 MB/sec output
3 AZ
pay per shard hour
pay per PUT payload unit
pay for extended data retention
pay for enhanced fanout


Kinesis firehose
synch rep across 3 AZ
pay for the volume of data ingested
pay for any data format conversions

DMS
use appropriate underlying EC2 instance to scale 
option to enable Multi-AZ
pay for computer resources 

Glue
specify approp # of DPUs
pay hourly rate billed by the second for crawlers + ETL jobs

###
Subdomain 1.3: Select a collection system that addresses the key properties of data, such as order, format, and compression
###

issues with incoming data
- out-of-order data
- duplicate data - increase burden on data stores, impacts accuracy + reliability of reports, increases system latency, and causes multiple updates of the same data

delivery + ordering with AWS
DDB streams - exactly once - supports guaranteed delivery
Kinesis data stream - at least once - supports guaranteed delivery
Kinesis data firehose - at least once - DOES NOT support guaranteed delivery
SQS standard - at least once - DOES NOT support guaranteed delivery
SQS FIFO - exactly once - supports guaranteed delivery 
Kafka/MSK - at least one - supports guaranteed delivery

other tools to transform + filter data during collection process
- kineis firehose
data transformation - lambda function processes each buffered batch asynch
can enable source record backup to backup untransformed records 
can also batch, compress, and encrypt data before loading data to destination
- lambda
- DMS - can transform before loading to target


########
Domain 2: Storage + Data Management
########
choose durable, performant, and cost-efficient storage

###
Subdomain 2.1: Determine the operational characteristics of the storage solution for analytics
###

Operational Data Stores (OLTP)
classic transactional ACID data stores
row-based
smaller compute size
low latency
high throughput
high concurrency
high change velocity
mission critical

RDS - fast, scales vertical, multi-AZ
DDB - fast, scales horiz, 3 AZ
ElastiCache (Redis, Memcache)- extremely fast, multi-AZ
Neptune (Graph) - fast, scales vert, 6 replicas across 3 AZ

vs

Analytic Data Stores
OLAP = realtime feedback + ad-hoc queries 
Decision Support Systems (DSS) = colder data in data lake + warehouse

columnar 
datasets are large + use partitioning 
large compute size
performs complex joins + aggs
bulk loading or trickle inserts
low change velocity

Redshift - data loads linearly, replicates data within warehouse + backs up into S3
S3

DDB
max item size 400KB
each item has variable attributes
each table must have at least a primary key - alt option is to have a composite primary key = primary + sort key
supports ACID transactions
1 RCU = 1 strongly consistent or 2 eventually consistent reads per second per item (up to 4KB in size)
1 WCU = 1 write per second per item (up to 1KB in size)

is the data hot or not?
hot > cold
elasticache > DDB/DAX > Aurora > RDS > Neptune > S3

###
Subdomain 2.2: Determine data access and retrieval patterns
###

for storing transient data = use in-memory data store
needed for immediate consumption
doesn't need to be highly durable bc quickly changing
caches trade off capacity for speed

for storing long-term data = OLTP and OLAP
DDB 
RDS
Redshift

storing archived data = glacier

Redshift block storage
choose the correct kind of drive for access patterns
SSD = random access
HDD = sequential access 

DC2 = dense compute = SSD
DS2 = dense storage = HDD although now recommend RA3
RA3 = pay for compute + storage independently - choose # of nodes needed for compute, and only pay for the storage that you use = SSD for compute + s3 for durable

###
Subdomain 2.3: Select appropriate data layout, schema, structure, and format
###

data storage optimizations
DDB
choose right partition key

storages all items with same partition key physically closed together, ordered by sort key (if provided)

global secondary index (GSI)
allows alt partition key + sort key vs main table
combo of primary + sort key does not require distinct values
"global" bc queries on index can span all data in the base table, across all partitions
up to 20 - but want to limit as much as possible
choose which attributes from the main table to project
stored sep from base table = customize throughput settings = will eat up sep RCU + WCU
can be created + deleted at any time
only support eventual consistency

local secondary index (LSI)
same partition key, diff sort key
local to the parition
limit 5 per table
all attributes from the base table are projected to the LSI
share RCU + WCU with base table
can only be created upon base table creation
cannot be deleted
supports eventual + strong consistency
limit to 10GB!

can compress large attributes values via GZIP > binary output > store as Binary attribute

Redshift cluster
1 leader node, 2+ compute nodes 
each compute has # of slices
each node slice = independent partition of data
slices perform operations in parallel

query that references only catalog tables (tables with PG prefix) runs exclusively on leader node

compute nodes execute queries + transmit data among themselves to serve these queries
intermediate results are returned to leader node + aggregated before returning 

# of node slices depends on node size
each DS1.XL = 2 slices
each DS1.8XL = 16 slices

optimize Redshift cluster
1. data distribution styles - EVEN, ALL, KEY, AUTO
2. sort key best practices
compound - default sort type - sort keys are prioritized by the order they are listed

interleaved - gives equal weight to each column in the sort key
don't use for columns with monotonically increasing attributes
basically don't use

3. compression encoding implementations - use AUTO
4. data size optimizations
split load into equal file sizes - between 1MB - 1GB after compression - ideal size is 1 - 125MB after compression
compress before loading

###
Subdomain 2.4: Define data lifecycle based on usage patterns and business requirements
###

S3 standard
S3 intelligent-tiering
S3 infrequent access
S3 glacier

transition actions = define when objects transition to another storage class
expiration actions = when object expires = S3 deletes expired objects for you

can take snapshots of RDS + Redshift
can backup entire DDB table

###
2.5: Determine the appropriate system for cataloging data and managing metadata
###
Glue data catalog = persistent metadata store
can be used for Athena, Redshift spectrum, + EMR

custom classifiers
https://docs.aws.amazon.com/glue/latest/dg/custom-classifier.html
if the crawler recognizes the data, it returns the classification + schema of the data to the crawler 
use case - want to customize the tables that are created by the crawler

crawlers can crawl data stores via JDBC connection:
Redshift 
RDS + Aurora
MariaDB, Microsoft SQL, MySQL, Oracle, PostgreSQL

in addition to S3 + DDB via their native interfaces

########
Domain 3: Processing
########

###
Subdomain 3.1: Determine appropriate data processing solution requirements
###
Glue triggers
scheduled - cron
job events (conditional) - event-based trigger that fires when a previous job or multiple jobs satisfy a list of conditions
CloudWatch events, Lambda, or step functions can trigger AWS Glue jobs

Spark libraries
Spark SQL - query structured data
Spark streaming - reuse for both streaming + batch processing - you can join streams against historical data or run ad-hoc queries on the stream
MLlib - machine learning 
GraphX - simply graph analytics tasks

EMR data source integrations
With EMRFS - EMR can use S3 as an object store for Hadoop - EMR streams the data from S3 to each instance in your cluster + begins processing immediately

###
Subdomain 3.2: Design a solution for transforming and preparing data for analysis
###
Batch vs stream
Static resources vs dynamic resources

###
Subdomain 3.3: Automate and operationalize a data processing solution
###
# Glue workflow
https://docs.aws.amazon.com/glue/latest/dg/orchestrate-using-workflows.html
set of related jobs/crawlers/triggers
provides visual rep of workflow as a graph in console
to share and manage state throughout workflow run, you candefine default workflow run properties 
these properties are available to all jobs in the workflow

# Automating AWS Glue ETL jobs with AWS Lambda
1. Build data lake in S3
Ingested data lands in S3 bucket = raw zone
2. To make that data available, you have to catalog its schema in AWS Glue Data Catalog
Lambda function with S3 trigger starts the glue crawler and catalogs the data
use SQS for event debugging + retries
3. when crawler is finished, invoke a second lambda function using a cloudwatch even rule that monitors when the crawler is done > starts Glue job to convert raw data into Parquet format and stores converted data into S3 bucket = processed zone
5. as soon as the ETL job finishes, another Cloudwatch event rule sends you an SNS alert notification when data is done processing 

# Using step functions with AWS Glue
Services orchestrated via AWS Step Functions
Ex: goal is to combine 2 diff data sets
1. S3 = landing zone for both data sets
2. CloudWatch events schedules the step function state machine to run weekly
3. when the step function state machine is triggered by a CloudWatch Event, it confirms the availability of the datasets via Lambda function
4. The step function state machine will then begin ETL job orchestration
5. Step functions will start 2 Glue ETL jobs in parallel - one to process each raw incoming data set
6. after the 2 Glue jobs are run, another AWS Glue ETL job combines the 2 data sets 

########
Domain 4: Analysis and visualization
########
use data you collected, processed, and transformed to generate actionable insights

white paper
https://d1.awsstatic.com/whitepapers/Big_Data_Analytics_Options_on_AWS.pdf

###
Subdomain 4.1: Determine the operational characteristics of an analysis and visualization solution
###
best and most cost-effective solution
decouple components - don't try to fit everything to work with one solution
can use same analytics tool on same data set to answer different questions 

Athena
interactive ad-hoc queries
cost - per TB queried
performance
- improve performance with compression, partitioning, and columnar format
durability + availability
- highly available
scalability + elasticity
- serverless
interfaces
- console, CLI, API, JDBC 
- create visuals in quicksight based on Athena queries - native when data is registered in AWS Glue Data Catalog

ElasticSearch
operational analytics
cost
- managed service, pay for what you use per ES instance hour, EBS storage (if used), and standard data transfer fees
- auto snapshots stored 14 days free, manual snapshots charged S3 storage rates
performance
- depends on
-- instance type
-- workload
-- index - made up of shards of data - can be distributed on diff instances in multiple AZ
-- # of shards used 
-- read replica config - replicas in diff AZ if zone awareness is checked
-- storage config (instance or EBS) - search engine makes heavy use of storage devices - fast SSD instance storage or multiple EBS volumes
durability + availability
- enable zone awareness - dist instances across 2 AZ
- enable replicas - instances auto distributed to deliver cross-zone replication
- daily auto snapshots, can be used to recover 
scalability + elasticity
- add/remove instances
- modify EBS volumes 
interfaces
- console, CLI, API
- integrations with Kibana and Logstash

EMR
big data analytics
cost
- pay for when cluster is up based on how many and what type EC2 instances chosen
performance
- type + # of EC2 instances
- need sufficient memory, storage, and processing power
- master node does not require much processing power
- core nodes need storage + processing power
- task nodes only need processing power
- launch cluster in same region as your data and other AWS resources
- avoid excessive logging to reduce processing load
durability + availability
- fault tolerant for core node failures - if core node fails, EMR will provision a new one
- however if all nodes in the cluster are lost, EMR will not replace them
scalability + elasticity
- resize cluster 
- add core nodes at any time for additional storage + processing
- add task nodes any time to add process power
- decouple memory + compute from storage - use S3 on EMRFS - greater flexibility + cost efficiency
interfaces
- Hive - HiveQL to query data > translates into a MR job - direct integration with DDB and S3
- Pig - Pig Latin scripting language over structured + unstructured data > translates into MR job
- Spark - in memory processing
- HBase - non-relational DB runs on top of Hadoop - stores lots of sparse data using column-based compression - fast lookup in-memory - optimized sequential writes - can backup to S3 
- Presto - SQL query for low-latency, ad-hoc analysis - process multiple data sources (including HDFS + S3)
- Kinesis connector - EMR read directly from Kinesis Data Streams for batch processing

Kinesis 
real-time analytics
perform time-series analytics, feed real-time dashboards, and create real-time metrics
cost 
- streams - hourly charge per shard + charge for each 1M PUT transactions
- firehose - amount of data ingested, extra charge for convert to parquet
- analytics - hourly rate based on avg # KPUs
- video - pay for volume you ingest, storage, and consume 
performance
- streams - # of shards, enhanced fanout
- firehose - batch size + interval + data compression
- analytics - # of KPUs
durability + availability
- fully managed
- kinesis data stream synchronously replicates data across 3 AZ, and stores up to 7 days
scalability + elasticity
- streams - dep on # of shards, can increase/decrease capacity at any time - but it's not instantaneous 
- firehose - autoscale
- analytics - increase # KPUs
- video - autoscale
interfaces
- stream - producers + consumers
- firehose - destinations - S3, Redshift, Splunk, ElasticSearch
- analytics - input options = stream or firehose - output to stream, firehose, or lambda
- video - use APIs 

Redshift 
run complex analytic queries against structured data 
and Redshift spectrum runs queries against structured + unstructured data
cost 
- charged on type and # of nodes
- no additional charge for backup storage to 100% of provisioned storage 
- excess storage is put into S3 + charged S3 rate
- no data transfer charge between S3 and Redshift
performance
- uses columnar storage, data compression, and zone maps to reduce I/O needed to perform queries 
- MPP architecture, parallelizing, and distributing SQL operations to take advantage of all available resources
- hardware designed for high performance data processing using locally attached storage
- 10 GigE mesh network to max throughput between nodes
- DS or DC nodes
- how you design, load, and cleanup the tables
durability + availability
- auto detects + replaces failed nodes
scalability + elasticity
- change # and type of nodes 
- elastic resize = change # of nodes
- classic resize = diff node types, takes long time - snapshot of existing cluster, restore snapshot in new cluster with desired node types, resize target cluster, direct traffic from original cluster to new target cluster 
interfaces
- JDBC/ODBC drivers

Sagemaker
predictive analytics
cost 
- pay for what you use - on-demand ML instances, ML storage, and fees for data processing
performance
- automatic autoscaling
- manually change # and type of instances
durability + availability
- fully managed, multi AZ
scalability + elasticity
- auto scaling
interfaces
- access notebook instances via console or API
- Python SDK

