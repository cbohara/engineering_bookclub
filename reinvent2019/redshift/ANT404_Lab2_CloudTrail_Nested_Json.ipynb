{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANT404 Lab #2: Query CloudTrail Nested JSON\n",
    "\n",
    "In this lab you will use Redshift Spectrum to query AWS CloudTrail logs. A bucket of example logs has been shared with your test account.\n",
    "\n",
    "**Background AWS CloudTrails Logs**\n",
    "\n",
    "AWS CloudTrail records activity on your AWS account and delivers log files to your Amazon S3 bucket. \n",
    "\n",
    "CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. \n",
    "\n",
    "CloudTrail logs are `JSON` data with nested data elements\n",
    "* Use the nested data syntax to define a table on the JSON data\n",
    "* Verify external table is working and query totals are correct\n",
    "* Review the table in the AWS Glue data catalog\n",
    "\n",
    "In this lab you will use Amazon Redshift Spectrum to query AWS CloudTrail logs. A bucket of example logs has been shared with your test account. The logs are `JSON` formatted data with nested data elements.\n",
    "\n",
    "Redshift Spectrum supports querying nested data in `Parquet`, `ORC`, `JSON`, and `Ion` file formats. You will use the nested data `DDL` syntax to define a table on the AWS CloudTrail logs.\n",
    "\n",
    "## 1. Check for credentials file\n",
    "Check for the credntials created in the `START_HERE` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"user_name\": \"ant404\",\n",
      "  \"password\": \"Pp-86feeb76\",\n",
      "  \"host_name\": \"10.0.54.136\",\n",
      "  \"port_num\": \"5439\",\n",
      "  \"db_name\": \"dev\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ant404-lab.creds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set local variables from credentials file\n",
    "Run this `cell` to import the credentials created in `START_HERE` notebook into this notebook. Later cells rely on these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson\n",
    "with open(\"ant404-lab.creds\") as fh:\n",
    "    creds = simplejson.loads(fh.read())\n",
    "username=creds[\"user_name\"]\n",
    "password=creds[\"password\"]\n",
    "host_name=creds[\"host_name\"]\n",
    "port_num=creds[\"port_num\"]\n",
    "db_name=creds[\"db_name\"]\n",
    "\n",
    "# Example Account, Region, and Cluster values for this lab\n",
    "account=123456789101\n",
    "region=\"us-east-1\"\n",
    "cluster_name=\"reporting-cluster\"\n",
    "\n",
    "# Default date values used to get sample files\n",
    "log_year=2019\n",
    "log_month=11\n",
    "log_day=10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set `env` shell variables with CloudTrail log location elements\n",
    "Run the `cell` to set these variables in the local shell. Do not quote the `%set_env` variable strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: username=ant404\n",
      "env: account=123456789101\n",
      "env: log_date=20191110\n",
      "env: log_date_path=year=2019/month=11/day=10\n",
      "env: bucket=redshift-managed-spectrum-datasets-us-east-1\n",
      "env: log_prefix=dataset=cloudtrail/region=us-east-1\n",
      "env: log_file=123456789101_CloudTrail_us-east-1_\n"
     ]
    }
   ],
   "source": [
    "%set_env username={username}\n",
    "%set_env account={account}\n",
    "\n",
    "# Default date value used to get sample files\n",
    "%set_env log_date={log_year}{log_month}{log_day}\n",
    "%set_env log_date_path=year={log_year}/month={log_month}/day={log_day}\n",
    "\n",
    "# S3 bucket for logs \n",
    "%set_env bucket=redshift-managed-spectrum-datasets-{region}\n",
    "\n",
    "# S3 prefix path for logs\n",
    "%set_env log_prefix=dataset=cloudtrail/region={region}\n",
    "# %set_env audit_prefix=cluster-audit-logs/AWSLogs/{account}/redshift/{region}\n",
    "\n",
    "# Log file name excluding date\n",
    "%set_env log_file={account}_CloudTrail_{region}_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List the CloudTrail log files\n",
    "Run the following `cell` to see a list of the first 15 CloudTrail log files in the specified S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudTrail files: \n",
      "s3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=10/123456789101_CloudTrail_us-east-1_\n",
      "-----------------------------\n",
      "2019-11-23 03:04:03       2410 123456789101_CloudTrail_us-east-1_20191110T2345Z_oLsrmUpSQJmbO0FU.json.gz\n",
      "2019-11-23 03:04:03      17165 123456789101_CloudTrail_us-east-1_20191110T2350Z_4bDyzDzErXPf3LZH.json.gz\n",
      "2019-11-23 03:04:03      29920 123456789101_CloudTrail_us-east-1_20191110T2350Z_6T7DePqM42RvbR4S.json.gz\n",
      "2019-11-23 03:04:03      27432 123456789101_CloudTrail_us-east-1_20191110T2350Z_GA0rwUEe0IZTAeUl.json.gz\n",
      "2019-11-23 03:04:03      25419 123456789101_CloudTrail_us-east-1_20191110T2350Z_IPm1w5JkXz7YlBSl.json.gz\n",
      "2019-11-23 03:04:04      18463 123456789101_CloudTrail_us-east-1_20191110T2350Z_Q3S65miz3Fwr3sNt.json.gz\n",
      "2019-11-23 03:04:04       1561 123456789101_CloudTrail_us-east-1_20191110T2350Z_koa8bosPcPyf46TD.json.gz\n",
      "2019-11-23 03:04:04      27668 123456789101_CloudTrail_us-east-1_20191110T2355Z_V6yaNeFqQMMS8ZGX.json.gz\n",
      "2019-11-23 03:04:04      19366 123456789101_CloudTrail_us-east-1_20191110T2355Z_e772hthTaqih0fFj.json.gz\n",
      "2019-11-23 03:04:04      24558 123456789101_CloudTrail_us-east-1_20191110T2355Z_j76h3LDaNSHDzS3W.json.gz\n",
      "2019-11-23 03:04:04      18785 123456789101_CloudTrail_us-east-1_20191110T2355Z_qU4wsbJtix5QfPjr.json.gz\n",
      "2019-11-23 03:04:04      32325 123456789101_CloudTrail_us-east-1_20191110T2355Z_zrNOAxluDDWrtSga.json.gz\n",
      "\n",
      "Total Objects: 1881\n",
      "   Total Size: 39606115\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# logpath=$bucket/$log_prefix/$log_date/$log_file\n",
    "# echo \"log files: \" $(aws s3 ls s3://$logpath | wc -l)\n",
    "# aws s3 ls s3://$logpath --human-readable | head -15\n",
    "log_full_prefix=$bucket/$log_prefix/$log_date_path/$log_file\n",
    "\n",
    "## Count and List the logs with the AWS CLI\n",
    "echo \"CloudTrail files: \"\n",
    "echo \"s3://$log_full_prefix\"\n",
    "echo \"-----------------------------\"\n",
    "aws s3 ls s3://$log_full_prefix --summarize | tail -15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download the one file to the notebook server "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "logpath=$bucket/$log_prefix/$log_date_path\n",
    "file=$log_file$log_date'T2355Z_zrNOAxluDDWrtSga.json.gz'\n",
    "aws s3 cp s3://$logpath/${file} ${file}\n",
    "gzip -df ${file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preview the file to determine the data structure\n",
    "\n",
    "Redshift CloudTrail logs use a complex JSON format with nested data elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"123456789101_CloudTrail_us-east-1_20191110T2355Z_zrNOAxluDDWrtSga.json\", 'r') as handle:\n",
    "    parsed = json.load(handle)\n",
    "# Preview the file\n",
    "print(json.dumps(parsed, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Identify nested data elements\n",
    "There are 2 key types of nested data elements\n",
    "* Scalar objects `{ }` where each field can be accessed with dot notation directly\n",
    "* Inline arrays `[ ]` which need to be declared as a table in your query to be accessed\n",
    "\n",
    "### Scalar Objects\n",
    "The `userIdentity` object is **scalar** because it contains no repeating elements. Fields are accessed directly using dot notation.  \n",
    "```json\n",
    "\"userIdentity\": {\n",
    "  \"type\": \"AssumedRole\",\n",
    "  \"accountId\": \"123456789101\",\n",
    "  \"sessionContext\": {\n",
    "    \"attributes\": {\n",
    "      \"mfaAuthenticated\": \"false\",\n",
    "      \"creationDate\": \"2019-11-12T18:30:46Z\" },\n",
    "    \"sessionIssuer\": {\n",
    "      \"type\": \"Role\",\n",
    "      \"arn\": \"arn:aws:iam::123456789101:role/EC2_InternalSite\",\n",
    "      \"accountId\": \"123456789101\",\n",
    "      \"userName\": \"EC2_InternalSite\" }\n",
    "  }\n",
    "},\n",
    "```\n",
    "\n",
    "### Inline Arrays\n",
    "By contrast, the data element `resources` contains an **array** of JSON objects. Fields are accessed by declaring the array as a sub-table in the `FROM` clause using PartiQL syntax.\n",
    "```json\n",
    "\"resources\": [\n",
    "   { \"ARN\": \"arn:aws:dynamodb:us-west-2:123456789101:table\",\n",
    "     \"accountId\": \"123456789101\",\n",
    "     \"type\": \"AWS::DynamoDB::Table\" },\n",
    "   { \"ARN\": \"arn:aws:kms:us-west-2:123456789101:key\",\n",
    "     \"accountId\": \"123456789101\",\n",
    "     \"type\": \"AWS::KMS::Key\" },\n",
    "   { \"ARN\": \"arn:aws:iam:us-west-2:123456789101:role\",\n",
    "     \"accountId\": \"123456789101\",\n",
    "     \"type\": \"AWS::IAM::Role\" }\n",
    " ],\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Connect to your Redshift cluster\n",
    "\n",
    "You will use the `sqlalchemy` and `ipython-sql` Python libraries to manage the Redshift connection. \n",
    "\n",
    "This cell creates a `%sql` element so we can use the connection in other cells in the notebook.\n",
    "\n",
    "-------\n",
    "_**Note:** Please ignore the pink error message that says: \"UserWarning: The psycopg2 wheel package will be renamed from release 2.8\"_'**Look for** 'Connected: ant404@dev' in the 'Out [ ]' section below the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import simplejson\n",
    "\n",
    "%reload_ext sql\n",
    "%config SqlMagic.displaylimit = 25\n",
    "\n",
    "connect_to_db = 'postgresql+psycopg2://'+username+':'+password+'@'+host_name+':'+port_num+'/'+db_name\n",
    "%sql $connect_to_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. List existing external `database`/`schema`/`table`\n",
    "These tables should now contain the tables created in Lab #1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM svv_external_databases WHERE databasename = 'logdata';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM svv_external_schemas WHERE schemaname = 'rawlogs';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM svv_external_tables WHERE schemaname = 'rawlogs' ORDER BY schemaname, tablename;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compose the external table DDL using nested data syntax\n",
    "External tables that reference nested JSON data use the following data types:\n",
    "\n",
    "* **Objects**\n",
    "    * JSON - `{\"field_1\": \"value_1\", \"field_2\": 2}`\n",
    "    * DDL - `struct< keyname:datatype [,keyname:datatype] >`\n",
    "* **Arrays**\n",
    "    * JSON - `[\\\"value_1\\\",\\\"value_2\\\"]`\n",
    "    * DDL - `array< datatype >`\n",
    "\n",
    "### Scalar Object DDL\n",
    "Each scalar object is declared as a `STRUCT < >` in Redshift Spectrum's nested data DDL syntax.\n",
    "```sql\n",
    "userIdentity        STRUCT <\n",
    "  type:               VARCHAR,\n",
    "  accountId:          VARCHAR,\n",
    "  sessionContext:       STRUCT <\n",
    "    attributes:           STRUCT <\n",
    "      mfaAuthenticated:     VARCHAR,\n",
    "      creationDate:         VARCHAR >,\n",
    "    sessionIssuer:        STRUCT <\n",
    "      type:                 VARCHAR,\n",
    "      arn:                  VARCHAR,\n",
    "      accountId:            VARCHAR,\n",
    "      userName:               VARCHAR > > >,\n",
    "```\n",
    "\n",
    "### Inline Array DDL\n",
    "```sql\n",
    "resources     ARRAY< STRUCT<\n",
    "  arn:          VARCHAR,\n",
    "  accountId:    VARCHAR,\n",
    "  type:         VARCHAR > >,\n",
    "```\n",
    "\n",
    "The first section of your DDL should look like this:\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE rawlogs.cloudtrail (\n",
    "    records             ARRAY<\n",
    "```\n",
    "\n",
    "\n",
    "Use the provided DDL to create an external table for this data set.  \n",
    "**Note that this DDL uses the external schema and database you created in Lab #1, Step 10.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "/* -- Escape autocommit with */END;/* -- */\n",
    "DROP TABLE IF EXISTS rawlogs.cloudtrail;\n",
    "CREATE EXTERNAL TABLE rawlogs.cloudtrail (\n",
    "    records             ARRAY< STRUCT<\n",
    "        eventversion:       VARCHAR(8),\n",
    "        useridentity:       STRUCT<\n",
    "            type:                   VARCHAR(16),\n",
    "            principalid:            VARCHAR(128),\n",
    "            arn:                    VARCHAR(256),\n",
    "            accountid:              VARCHAR(16),\n",
    "            invokedby:              VARCHAR(64),\n",
    "            accesskeyid:            VARCHAR(32),\n",
    "            username:               VARCHAR(32),\n",
    "            sessioncontext:         STRUCT<\n",
    "                attributes:             STRUCT<\n",
    "                    mfaauthenticated:       VARCHAR(8),\n",
    "                    creationdate:           VARCHAR(32)>,\n",
    "                sessionissuer:          STRUCT<\n",
    "                    type:                   VARCHAR(8),\n",
    "                    principalid:            VARCHAR(32),\n",
    "                    arn:                    VARCHAR(256),\n",
    "                    accountid:              VARCHAR(16),\n",
    "                    username:                   VARCHAR(64) > > >,\n",
    "        eventtime:           VARCHAR(32),\n",
    "        eventsource:         VARCHAR(64),\n",
    "        eventname:           VARCHAR(64),\n",
    "        awsregion:           VARCHAR(16),\n",
    "        sourceipaddress:     VARCHAR(64),\n",
    "        useragent:           VARCHAR(256),\n",
    "        errorcode:           VARCHAR(64),\n",
    "        errormessage:        VARCHAR(512),\n",
    "        requestparameters:   STRUCT<\n",
    "            durationseconds:    INTEGER,\n",
    "            rolearn:            VARCHAR(256),\n",
    "            rolesessionname:    VARCHAR(64),\n",
    "            partitionValues:    ARRAY<VARCHAR>,\n",
    "            databaseName:       VARCHAR(16),\n",
    "            tableName:          VARCHAR(64) >,\n",
    "        responseelements:    STRUCT<\n",
    "            assumedRoleUser:    STRUCT<\n",
    "                arn:                VARCHAR(128),\n",
    "                assumedRoleId:      VARCHAR(64) >,\n",
    "            \"credentials\":      STRUCT<\n",
    "                accessKeyId:        VARCHAR(32),\n",
    "                expiration:         VARCHAR(32),\n",
    "                sessionToken:       VARCHAR(2048) > >,\n",
    "        additionaleventdata: STRUCT<\n",
    "            insufficientLakeFormationPermissions: ARRAY<VARCHAR >,\n",
    "            lakeFormationPrincipal: VARCHAR(128) >,\n",
    "        requestid:           VARCHAR(64),\n",
    "        eventid:             VARCHAR(64),\n",
    "        resources:           ARRAY< STRUCT<\n",
    "            arn:                           VARCHAR(256),\n",
    "            accountid:                     VARCHAR(16),\n",
    "            type:                          VARCHAR(32) > >,\n",
    "        eventtype:           VARCHAR(32),\n",
    "        apiversion:          VARCHAR(16),\n",
    "        readonly:            VARCHAR(8),\n",
    "        recipientaccountid:  VARCHAR(16),\n",
    "        serviceeventdetails: VARCHAR(1024),\n",
    "        sharedeventid:       VARCHAR(64),\n",
    "        vpcendpointid:       VARCHAR(16) > >\n",
    ") \n",
    "PARTITIONED BY (\n",
    "      region VARCHAR(32)\n",
    "    , log_year INT\n",
    "    , log_month INT\n",
    "    , log_day INT\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/cloudtrail/'\n",
    "TABLE PROPERTIES ('compression_type'='gzip')\n",
    ";\n",
    "SELECT * FROM svv_external_tables WHERE schemaname = 'rawlogs' AND tablename = 'cloudtrail';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Add partitions for each day\n",
    "There is no data associated with a partitioned table until at least one partition is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Escape autocommit with */END;/* -- */\n",
    "ALTER TABLE rawlogs.cloudtrail \n",
    "ADD IF NOT EXISTS\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=1 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=01/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=2 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=02/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=3 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=03/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=4 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=04/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=5 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=05/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=6 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=06/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=7 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=07/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=8 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=08/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=9 ) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=09/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=10) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=10/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=11) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=11/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=12) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=12/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=13) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=13/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=14) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=14/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=15) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=15/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=16) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=16/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=17) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=17/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=18) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=18/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=19) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=19/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=20) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=20/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=21) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=21/'\n",
    "    PARTITION (region='us-east-1', log_year=2019, log_month=11, log_day=22) LOCATION 's3://redshift-managed-spectrum-datasets-us-east-1/dataset=cloudtrail/region=us-east-1/year=2019/month=11/day=22/'\n",
    ";\n",
    "SELECT * FROM svv_external_partitions WHERE schemaname = 'rawlogs' AND tablename = 'cloudtrail' ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Verify external table works - Partition Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT rec.eventSource \n",
    "     , log.region \n",
    "     , COUNT(*) \n",
    "FROM rawlogs.cloudtrail log \n",
    "   , log.records rec \n",
    "WHERE log.log_day = 9\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 1,2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Verify external table works - Scalar Object Queries\n",
    "The struct fields are accessed directly using dot notation. For example, the following query accesses the fields from the `userIdentity` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT rec.userIdentity.type\n",
    "     , rec.userIdentity.principalId\n",
    "     , rec.userIdentity.arn\n",
    "     , rec.userIdentity.accountId\n",
    "     , rec.userIdentity.invokedBy\n",
    "     , rec.userIdentity.accessKeyId\n",
    "     , rec.userIdentity.userName\n",
    "     , rec.userIdentity.sessionContext.attributes.mfaAuthenticated\n",
    "     , rec.userIdentity.sessionContext.attributes.creationDate\n",
    "     , rec.userIdentity.sessionContext.sessionIssuer.type\n",
    "     , rec.userIdentity.sessionContext.sessionIssuer.principalId\n",
    "     , rec.userIdentity.sessionContext.sessionIssuer.arn\n",
    "     , rec.userIdentity.sessionContext.sessionIssuer.accountId\n",
    "     , rec.userIdentity.sessionContext.sessionIssuer.userName\n",
    "FROM rawlogs.cloudtrail log \n",
    "   , log.records rec -- # Top level inline \"Records\" array\n",
    "WHERE log.region = 'us-east-1' \n",
    "  AND log.log_year = 2019 \n",
    "  AND log.log_month = 11 \n",
    "  AND log.log_day = 13 \n",
    "  AND rec.userIdentity.invokedBy <> 'dynamodb.application-autoscaling.amazonaws.com'\n",
    "  AND rec.userIdentity.sessionContext.attributes.mfaAuthenticated IS NOT NULL\n",
    "LIMIT 25;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Inline Array Queries - `INNER JOIN`\n",
    "To query the objects in the nested array we declared it as a table and use an inner join. Note that rows will excluded if the array is empty for that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT rec.eventSource\n",
    "     , log.region \n",
    "     , arn.type\n",
    "     , COUNT(*)\n",
    "FROM rawlogs.cloudtrail log \n",
    "   , log.records rec   -- # Top level inline \"Records\" array\n",
    "   , rec.resources arn -- # \"Resources\" inline table array\n",
    "WHERE log.log_year = 2019 \n",
    "  AND log.log_month = 11 \n",
    "  AND log.log_day = 9\n",
    "GROUP BY 1,2,3\n",
    "ORDER BY 1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Inline Array Queries - `OUTER JOIN`\n",
    "To get all rows, even when the inline array is empty, use a left outer join when declaring the array as a table. If the array is empty that row will not appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT rec.eventSource\n",
    "     , log.region \n",
    "     , arn.type\n",
    "     , COUNT(*)\n",
    "FROM rawlogs.cloudtrail log \n",
    "JOIN log.records rec   ON true     -- # Top level inline \"Records\" array\n",
    "LEFT JOIN rec.resources arn ON true -- # \"Resources\" inline table array\n",
    "WHERE log.log_year = 2019 \n",
    "  AND log.log_month = 11 \n",
    "  AND log.log_day = 9 \n",
    "GROUP BY 1,2,3\n",
    "ORDER BY 1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Info on AWS CloudTrail Logs\n",
    "* Redshift Documentation: [\"Logging Amazon Redshift API Calls with AWS CloudTrail\"](https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html#rs-db-auditing-cloud-trail)\n",
    "* AWS Documentation: [\"Querying AWS CloudTrail Logs\"](https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html)\n",
    "* AWS Labs (on GitHub): [\"AthenaGlueServiceLogs\"](https://github.com/awslabs/athena-glue-service-logs)'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
