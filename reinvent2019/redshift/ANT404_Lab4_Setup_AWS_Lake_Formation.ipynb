{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANT404 Lab #4: Setup AWS Lake Formation\n",
    "\n",
    "### Why use AWS Lake Formation?\n",
    "\n",
    "AWS Lake Formation makes it easier to build, secure, and manage a data lake. It simplifies and automates the steps that required to create a data lake, especially cataloging data and making data available securely for analytics and machine learning.\n",
    "\n",
    "Lake Formation provides a permissions model that can be enforced at the table and column level and works across the full portfolio of AWS analytics and machine learning services, including Amazon Redshift Spectrum and Amazon Athena.\n",
    "\n",
    "This centrally defined permissions model enables fine-grained access to data stored in your data lake through a simple grant/revoke mechanism.\n",
    "\n",
    "The AWS Glue Data Catalog integrates the data access policies, making sure of compliance regardless of the dataâ€™s origin. \n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/lake-formation/latest/dg/images/overview-diagram.png\" width=\"700\"  />\n",
    "\n",
    "\n",
    "\n",
    "### Steps for creating a Lake Formation Data Lake\n",
    "You will complete the following steps to create your Lake Formation data lake.\n",
    "* Attach `AWSLakeFormationDataAdmin` policy to your current user\n",
    "* Create a new policy and role for Redshift to access Lake Formation and Glue\n",
    "* Copy your parquet data to a new Data Lake bucket\n",
    "* Create a new external database and new tables in AWS Glue\n",
    "* Register the bucket with Lake Formation and grant `SELECT` table permissions\n",
    "* Create a Redshift external schema for the Lake Formation data lake\n",
    "* Check that the restricted columns are _NOT_ visible in new tables\n",
    "* Compare the output of the previous tables to new Lake Formation tables\n",
    "\n",
    "\n",
    "## 1. Check for credentials file\n",
    "Check for the credentials created in the `START_HERE` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat ant404-lab.creds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set local variables from credentials file\n",
    "Run this `cell` to import the credentials created in `START_HERE` notebook into this notebook. Later cells rely on these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson\n",
    "with open(\"ant404-lab.creds\") as fh:\n",
    "    creds = simplejson.loads(fh.read())\n",
    "username=creds[\"user_name\"]\n",
    "password=creds[\"password\"]\n",
    "host_name=creds[\"host_name\"]\n",
    "port_num=creds[\"port_num\"]\n",
    "db_name=creds[\"db_name\"]\n",
    "\n",
    "# Example Account, Region, and Cluster values for this lab\n",
    "log_account=123456789101\n",
    "region=\"us-east-1\"\n",
    "cluster_name=\"reporting-cluster\"\n",
    "\n",
    "# Default date values used to get sample files\n",
    "audit_year=2019\n",
    "audit_month=11\n",
    "audit_day=10 \n",
    "\n",
    "%set_env username={username}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a `DataLakeUserPolicy` policy\n",
    "This will allow Lake Formation access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "policy='{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [{\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Action\": [\n",
    "      \"lakeformation:GetDataAccess\",\n",
    "      \"glue:GetTable\",\n",
    "      \"glue:GetTables\",\n",
    "      \"glue:SearchTables\",\n",
    "      \"glue:GetDatabase\",\n",
    "      \"glue:GetDatabases\",\n",
    "      \"glue:GetPartitions\"],\n",
    "    \"Resource\": \"*\"}]}'\n",
    "\n",
    "aws iam create-policy --policy-name DataLakeUserPolicy --policy-document \"$policy\"\n",
    "\n",
    "aws iam get-policy --policy-arn arn:aws:iam::080945919444:policy/DataLakeUserPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a new `RedshiftDataLakeUserRole` role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "trust_policy='{\n",
    "\"Version\": \"2012-10-17\",\n",
    "\"Statement\": [{\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\"Service\": \"redshift.amazonaws.com\"},\n",
    "      \"Action\": \"sts:AssumeRole\",\n",
    "      \"Condition\": {}\n",
    "}]}'\n",
    "\n",
    "aws iam create-role --role-name RedshiftDataLakeUserRole --assume-role-policy-document \"$trust_policy\"\n",
    "\n",
    "aws iam get-role --role-name RedshiftDataLakeUserRole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attach the `DataLakeUserPolicy` to the role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws iam attach-role-policy --role-name RedshiftDataLakeUserRole \\\n",
    "    --policy-arn arn:aws:iam::080945919444:policy/DataLakeUserPolicy\n",
    "    \n",
    "aws iam list-attached-role-policies --role-name RedshiftDataLakeUserRole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Associate the `RedshiftDataLakeUserRole` role to your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws redshift modify-cluster-iam-roles --cluster-identifier mod-27c4c61fae3b42fe-redshiftcluster-bz825ah27i69 \\\n",
    "    --add-iam-roles arn:aws:iam::080945919444:role/RedshiftDataLakeUserRole \\\n",
    "    --query 'Cluster.IamRoles[]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Copy your parquet data to a new Data Lake bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 mb s3://ant404-datalake-86feeb76\n",
    "aws s3 cp s3://ant404-lab-86feeb76/data_lake s3://ant404-datalake-86feeb76/ \\\n",
    "    --recursive --acl bucket-owner-full-control\n",
    "aws s3 ls s3://ant404-datalake-86feeb76/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create a new external database and tables in AWS Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws glue create-database --database-input '{\"Name\": \"lakeformation\",\"CreateTableDefaultPermissions\": []}'\n",
    "aws glue get-database --name \"lakeformation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Create `useractivitylog` table\n",
    "This structured JSON table definition for Glue can retrieved from an existing table using:  \n",
    "`aws glue get-table --database-name \"glue_db\" --name \"tbl_name\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tabledef='{ \"Name\": \"useractivitylog\",\n",
    "\"StorageDescriptor\": {\n",
    "    \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "    \"InputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "    \"SerdeInfo\": {\n",
    "        \"SerializationLibrary\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\n",
    "        \"Parameters\": {\"serialization.format\": \"1\" } },\n",
    "    \"Location\": \"s3://ant404-datalake-86feeb76/table=useractivitylog/region=us-east-1/log_year=2019/log_month=11/\",\n",
    "    \"Columns\": [\n",
    "        { \"Type\": \"varchar(32)\", \"Name\": \"recordtime\" },\n",
    "        { \"Type\": \"varchar(64)\", \"Name\": \"db\" },\n",
    "        { \"Type\": \"varchar(64)\", \"Name\": \"username\" },\n",
    "        { \"Type\": \"bigint\", \"Name\": \"pid\" },\n",
    "        { \"Type\": \"int\", \"Name\": \"userid\" },\n",
    "        { \"Type\": \"bigint\", \"Name\": \"xid\" },\n",
    "        { \"Type\": \"varchar(65535)\", \"Name\": \"query\" },\n",
    "        { \"Type\": \"int\", \"Name\": \"log_day\" } ]\n",
    "}\n",
    "}'\n",
    "aws glue create-table --database-name \"lakeformation\" --table-input \"$tabledef\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Create `connectionlog` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tabledef='{ \"Name\": \"connectionlog\",\n",
    "  \"StorageDescriptor\": {\n",
    "      \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "      \"InputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "      \"SerdeInfo\": {\n",
    "          \"SerializationLibrary\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\n",
    "          \"Parameters\": {\"serialization.format\": \"1\" }},\n",
    "      \"Location\": \"s3://ant404-datalake-86feeb76/table=connectionlog/region=us-east-1/log_year=2019/log_month=11/\",\n",
    "      \"Columns\": [\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"event\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"recordtime\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"remotehost\" },\n",
    "          { \"Type\": \"int\", \"Name\": \"remoteport\" },\n",
    "          { \"Type\": \"int\", \"Name\": \"pid\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"dbname\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"username\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"authmethod\" },\n",
    "          { \"Type\": \"bigint\", \"Name\": \"duration\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"sslversion\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"sslcipher\" },\n",
    "          { \"Type\": \"int\", \"Name\": \"mtu\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"sslcompression\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"sslexpansion\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"iamauthguid\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"application_name\" },\n",
    "          { \"Type\": \"int\", \"Name\": \"log_day\" } ]\n",
    "      }\n",
    "}'\n",
    "aws glue create-table --database-name \"lakeformation\" --table-input \"$tabledef\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Create `cloudtrail` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tabledef='{ \"Name\": \"cloudtrail\",\n",
    "  \"StorageDescriptor\": {\n",
    "      \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "      \"InputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "      \"SerdeInfo\": {\n",
    "          \"SerializationLibrary\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\n",
    "          \"Parameters\": {\"serialization.format\": \"1\"}},\n",
    "      \"Location\": \"s3://ant404-datalake-86feeb76/table=cloudtrail/region=us-east-1/log_year=2019/log_month=11/\",\n",
    "      \"Columns\": [\n",
    "          { \"Type\": \"varchar(8)\", \"Name\": \"event_version\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"user_identity_type\" },\n",
    "          { \"Type\": \"varchar(128)\", \"Name\": \"user_identity_principalid\" },\n",
    "          { \"Type\": \"varchar(256)\", \"Name\": \"user_identity_arn\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"user_identity_accountid\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"user_identity_invokedby\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"user_identity_accesskeyid\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"user_identity_username\" },\n",
    "          { \"Type\": \"varchar(8)\", \"Name\": \"session_context_mfa_authenticated\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"session_context_creation_date\" },\n",
    "          { \"Type\": \"varchar(8)\", \"Name\": \"session_issuer_type\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"session_issuer_principal_id\" },\n",
    "          { \"Type\": \"varchar(256)\", \"Name\": \"session_issuer_arn\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"session_issuer_account_id\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"session_issuer_user_name\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"event_time\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"event_source\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"event_name\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"aws_region\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"source_ipaddress\" },\n",
    "          { \"Type\": \"varchar(256)\", \"Name\": \"user_agent\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"error_code\" },\n",
    "          { \"Type\": \"varchar(512)\", \"Name\": \"error_message\" },\n",
    "          { \"Type\": \"int\", \"Name\": \"request_param_duration_seconds\" },\n",
    "          { \"Type\": \"varchar(256)\", \"Name\": \"request_param_role_arn\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"request_param_role_session_name\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"request_param_database_name\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"request_param_table_name\" },\n",
    "          { \"Type\": \"varchar(128)\", \"Name\": \"assumed_role_user_arn\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"assumed_role_user_assumed_role_id\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"credentials_access_key_id\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"credentials_expiration\" },\n",
    "          { \"Type\": \"varchar(2048)\", \"Name\": \"credentials_session_token\" },\n",
    "          { \"Type\": \"varchar(128)\", \"Name\": \"lake_formation_principal\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"request_id\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"event_id\" },\n",
    "          { \"Type\": \"varchar(256)\", \"Name\": \"resource_arn\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"resource_accountid\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"resource_type\" },\n",
    "          { \"Type\": \"varchar(32)\", \"Name\": \"event_type\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"api_version\" },\n",
    "          { \"Type\": \"varchar(8)\", \"Name\": \"read_only\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"recipient_account_id\" },\n",
    "          { \"Type\": \"varchar(1024)\", \"Name\": \"service_event_details\" },\n",
    "          { \"Type\": \"varchar(64)\", \"Name\": \"shared_event_id\" },\n",
    "          { \"Type\": \"varchar(16)\", \"Name\": \"vpc_endpoint_id\" },\n",
    "          { \"Type\": \"int\", \"Name\": \"log_day\" } ]\n",
    "    }\n",
    "}'\n",
    "aws glue create-table --database-name \"lakeformation\" --table-input \"$tabledef\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. List the tables in the AWS Glue database to confirm they were created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws glue get-tables --database-name \"lakeformation\" --query 'TableList[].{DbName:DatabaseName,TableName:Name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Register the bucket with Lake Formation and set table permissions\n",
    "\n",
    "In order to restrict access. You must register the S3 location with Lake Formation as a \"resource\". Then you grant `SELECT` permissions on that resource to the `RedshiftDataLakeUserRole`. \n",
    "\n",
    "\n",
    "\n",
    "### 9.1. Register the Data Lake bucket\n",
    "------\n",
    "**DO NOT RUN** this step until you have created the Glue tables above. Once you register the resource you will need to either grant additional explicit permissions or de-register it to make further changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws lakeformation register-resource \\\n",
    "    --resource-arn \"arn:aws:s3:::ant404-datalake-86feeb76/\" \\\n",
    "    --use-service-linked-role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Define the `DataLakePrincipalIdentifier` \n",
    "This is the authorized identity that accesses the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal='''{\"DataLakePrincipalIdentifier\":\"arn:aws:iam::080945919444:role/RedshiftDataLakeUserRole\"}'''\n",
    "%set_env principal={principal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Register `cloudtrail` and exclude 2 `credentials_` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cloudtrail='{\"TableWithColumns\":{\n",
    "    \"DatabaseName\":\"lakeformation\",\n",
    "    \"Name\":\"cloudtrail\",\n",
    "    \"ColumnWildcard\":{\"ExcludedColumnNames\":[\"credentials_access_key_id\",\"credentials_session_token\"]}}}'\n",
    "aws lakeformation grant-permissions --principal \"$principal\" --resource \"$cloudtrail\" --permissions '[\"SELECT\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4. Register `useractivitylog` and exclude the `query` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "useractivitylog='{\"TableWithColumns\":{\n",
    "    \"DatabaseName\":\"lakeformation\",\n",
    "    \"Name\":\"useractivitylog\",\n",
    "    \"ColumnWildcard\":{\"ExcludedColumnNames\":[\"query\"]}}}'\n",
    "aws lakeformation grant-permissions --principal \"$principal\" --resource \"$useractivitylog\" --permissions '[\"SELECT\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5. Register `connectionlog` and exclude the `iamauthguid` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "connectionlog='{\"TableWithColumns\":{\n",
    "    \"DatabaseName\":\"lakeformation\",\n",
    "    \"Name\":\"connectionlog\",\n",
    "    \"ColumnWildcard\":{\"ExcludedColumnNames\":[\"iamauthguid\"]}}}'\n",
    "aws lakeformation grant-permissions --principal \"$principal\" --resource \"$connectionlog\" --permissions '[\"SELECT\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Connect to your Redshift cluster\n",
    "\n",
    "You will use the `sqlalchemy` and `ipython-sql` Python libraries to manage the Redshift connection. \n",
    "\n",
    "This cell creates a `%sql` element so we can use the connection in other cells in the notebook.\n",
    "\n",
    "-------\n",
    "**Note:** _Please ignore the pink error message that says: \"UserWarning: The psycopg2 wheel package will be renamed from release 2.8\"_   \n",
    "**Look for** 'Connected: ant404@dev' in the 'Out [ ]' section below the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import simplejson\n",
    "\n",
    "%reload_ext sql\n",
    "%config SqlMagic.displaylimit = 25\n",
    "\n",
    "connect_to_db = 'postgresql+psycopg2://'+username+':'+password+'@'+host_name+':'+port_num+'/'+db_name\n",
    "%sql $connect_to_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 11. Create a Redshift external schema for the Data Lake\n",
    "You are creating a separate external schema for Data Lake user. This schema uses the new IAM role with restricted access permissions. You can then `GRANT` this restricted access to specific Redshift users and groups. For instance you may want allow Analysts to query the data lake but restrict access to data fields that contain PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE EXTERNAL SCHEMA IF NOT EXISTS lakeformation\n",
    "FROM DATA CATALOG\n",
    "DATABASE 'lakeformation'\n",
    "IAM_ROLE 'arn:aws:iam::080945919444:role/RedshiftDataLakeUserRole'\n",
    ";\n",
    "SELECT * FROM svv_external_schemas WHERE schemaname = 'lakeformation';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Check that the restricted columns are _NOT_ visible\n",
    "\n",
    "---\n",
    "**NOTE** This query _should_ return no rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT schemaname, tablename, columnname\n",
    "FROM svv_external_columns\n",
    "WHERE schemaname = 'lakeformation'\n",
    "  AND tablename IN ('cloudtrail','connectionlog','useractivitylog')\n",
    "-- # Excluded columns list\n",
    "  AND columnname IN ('iamauthguid','query','credentials_access_key_id','credentials_session_token')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM lakeformation.useractivitylog LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM lakeformation.connectionlog LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM lakeformation.cloudtrail LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Compare the output of the Admin access tables to new Data Lake tables\n",
    "\n",
    "###  Test the Data Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT user_identity_principalid, COUNT(*)\n",
    "FROM  lakeformation.cloudtrail\n",
    "WHERE log_year = 2019 \n",
    "  AND log_month = 11 \n",
    "  AND log_day = 11 \n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 10\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the table table with full Admin access\n",
    "This is the view created in Lab #3 over the table created in Lab #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT user_identity_principalid, COUNT(*)\n",
    "FROM  public.v_export_cloudtrail\n",
    "WHERE log_year = 2019 \n",
    "  AND log_month = 11 \n",
    "  AND log_day = 11 \n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 10\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Info on Lake Formation\n",
    "\n",
    "* Lake Formation Documentation: [\"What Is AWS Lake Formation?\"](https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html)\n",
    "* Lake Formation Tutorial: [Creating a Data Lake from an AWS CloudTrail Source](https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started-cloudtrail-tutorial.html)\n",
    "* AWS Blog: [Getting started with AWS Lake Formation](https://aws.amazon.com/blogs/big-data/getting-started-with-aws-lake-formation/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
